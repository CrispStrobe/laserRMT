{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVVMA_XfGDAM"
      },
      "source": [
        "### **Semantic Performance LASER - From text perplexity to task performance.**\n",
        "\n",
        "by [Lucas HÃ¤nke de Cansino](https://www.linkedin.com/in/lucas-h%C3%A4nke-de-cansino-8b8521234/), [Fernando Fernandes Neto](https://twitter.com/FernandoNetoAi), [David Golchinfar](https://twitter.com/DavidGFar) and [Eric Hartford](https://twitter.com/erhartford)\n",
        "\n",
        "With this notebook, we present a novel approach to applying LaserRMT shifting from perplexity based assessment of the lasered model to text generation performance on datasets previously seen during training. This method leverage similarity based scoring of generated answers against both chosen and rejected answers from Direct Preference Optimization (DPO) datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5lajSuSO27A"
      },
      "source": [
        "**Overview:**\n",
        "\n",
        "Here, we provide an exemplary demonstration of what the approach looks like on `openaccess-ai-collective/DPOpenHermes-7B-v2` and it's two DPO training datasets `Intel/orca_dpo_pairs` and `allenai/ultrafeedback_binarized_cleaned`. To give you a brief overview of the process: Initially, the script is executed, which, among other outputs, generates a JSON file containing the current top 16 highest SNR/max singular value for each module in every layer. Following this, we will guide you on how to use the extracted layers in Axolotl or LlamaFactory for your training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJhqNXDPPAuC"
      },
      "source": [
        "### **The laser-scanner script:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aZQkBb0FpPc"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from lib.utils.load_benchmark_dataset import get_benchmark_data\n",
        "\n",
        "from lib.utils.assets import PromptTemplate\n",
        "from lib.utils.prompt_template import get_llm_prompt\n",
        "\n",
        "from lib.utils.AutoModelForSentenceEmbedding import (\n",
        "    AutoModelForSentenceEmbedding,\n",
        "    get_cosine_embeddings,\n",
        ")\n",
        "\n",
        "class ModelModifier:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name,\n",
        "        prompt_template: PromptTemplate = PromptTemplate.chatml,\n",
        "        input_length=512,\n",
        "        output_length=512,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.prompt_template = prompt_template\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.bfloat16, device_map={\"\": 0}\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, use_fast=True\n",
        "        )\n",
        "        self.layer_snr = {}\n",
        "        self.modified_layers = set()\n",
        "        self.original_weights = {}\n",
        "        self.input_length = input_length\n",
        "        self.output_length = output_length\n",
        "        self.embeddings_model = AutoModelForSentenceEmbedding(\n",
        "            model_name, self.tokenizer\n",
        "        )\n",
        "\n",
        "    def calculate_snr_for_layer(self, layer_type, layer_number):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if layer_type in name and str(layer_number) in name:\n",
        "                weights = module.weight.double()\n",
        "                S = torch.linalg.svdvals(weights)\n",
        "                weights = weights.detach().cpu()\n",
        "                S = S.detach().cpu()\n",
        "                sigma_estimated = self.estimate_sigma_with_full_iqr(S)\n",
        "                n, m = weights.shape\n",
        "                mp_threshold = self.marchenko_pastur_threshold(sigma_estimated, n, m)\n",
        "\n",
        "                signal = S[S > mp_threshold].sum()\n",
        "                noise = S[S <= mp_threshold].sum()\n",
        "                snr = signal / noise if noise != 0 else float(\"inf\")\n",
        "                del S, weights\n",
        "                torch.cuda.empty_cache()  # Clear PyTorch's CUDA memory cache\n",
        "                gc.collect()\n",
        "                return snr\n",
        "\n",
        "    def update_model_reduce_layer(self, layer_type, layer_number):\n",
        "        layer_id = f\"{layer_type}+{layer_number}\"\n",
        "        if layer_id in self.modified_layers:\n",
        "            print(f\"Layer {layer_id} has already been modified. Skipping.\")\n",
        "            return False\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if layer_type in name and str(layer_number) in name:\n",
        "                print(f\"Reconstructing layer: {name}\")\n",
        "                original_dtype = module.weight.dtype\n",
        "                self.original_weights[name] = module.weight.detach().clone()\n",
        "                weights = module.weight.double()\n",
        "                U, S, V = torch.linalg.svd(weights, full_matrices=False)\n",
        "\n",
        "                # Estimate sigma using the full IQR method\n",
        "                sigma_estimated_full_iqr = self.estimate_sigma_with_full_iqr(S)\n",
        "\n",
        "                # Calculate Marchenko-Pastur threshold\n",
        "                n, m = weights.shape\n",
        "                mp_threshold_full_iqr = self.marchenko_pastur_threshold(\n",
        "                    sigma_estimated_full_iqr, n, m\n",
        "                )\n",
        "\n",
        "                # Retain only the singular values above the MP threshold\n",
        "                S_reduced = torch.zeros_like(S)\n",
        "                k = (S > mp_threshold_full_iqr).sum().item()\n",
        "                S_reduced[:k] = S[:k]\n",
        "                print(f\"Reduced from {S.shape} to {k}\")\n",
        "\n",
        "                # Reconstruct the matrix using the thresholded singular values\n",
        "                reconstructed_weights = U @ torch.diag(S_reduced) @ V\n",
        "                reconstructed_weights = reconstructed_weights.to(original_dtype)\n",
        "                module.weight = torch.nn.Parameter(reconstructed_weights)\n",
        "                self.modified_layers.add(layer_id)\n",
        "                return True\n",
        "\n",
        "    @staticmethod\n",
        "    def marchenko_pastur_threshold(sigma, n, m):\n",
        "        beta = n / m if n < m else m / n\n",
        "        threshold = sigma * np.sqrt((1 + np.sqrt(beta)) ** 2)\n",
        "        return threshold\n",
        "\n",
        "    ## Calculate an estimate of the standard deviation of the singular values based on Inter Quantile Range\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate_sigma_with_full_iqr(S):\n",
        "        q75 = torch.quantile(S, 0.75)\n",
        "        q25 = torch.quantile(S, 0.25)\n",
        "        iqr = q75 - q25\n",
        "        sigma_estimated = (\n",
        "            iqr / 1.349\n",
        "        )  ## 0.6745 * sigma is the expected range between the quantiles (Q1 and Q3)\n",
        "        return sigma_estimated\n",
        "\n",
        "    def restore_model_original_layer(self, layer_type, layer_number):\n",
        "        layer_id = f\"{layer_type}+{layer_number}\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if layer_type in name and layer_number in name:\n",
        "                if name in self.original_weights:\n",
        "                    module.weight = torch.nn.Parameter(self.original_weights[name])\n",
        "                    print(f\"Restored original weights for layer: {name}\", flush=True)\n",
        "                    if layer_id in self.modified_layers:\n",
        "                        self.modified_layers.remove(layer_id)\n",
        "                        break\n",
        "                else:\n",
        "                    print(f\"No original weights saved for layer: {name}\", flush=True)\n",
        "        return\n",
        "\n",
        "    def calculate_model_performance(\n",
        "        self,\n",
        "        datasets=[\"orca_dpo\", \"ultrafeedback\"],  # \"openhermes\"\n",
        "        n_samples=128,\n",
        "        input_length=512,\n",
        "        output_length=512,\n",
        "    ):\n",
        "        score_accumulated = 0.0\n",
        "        model = self.model\n",
        "        tokenizer = self.tokenizer\n",
        "        embeddings_model = self.embeddings_model\n",
        "        for dataset in datasets:\n",
        "            benchmark_dataset = get_benchmark_data(\n",
        "                dataset, n_samples, input_length, output_length\n",
        "            )\n",
        "            print(\"Calculating performance for dataset:\", dataset)\n",
        "            for index, sample in enumerate(benchmark_dataset.data):\n",
        "                progress = str(f\"{index}/{n_samples}\")\n",
        "                print(progress)\n",
        "                prompt = get_llm_prompt(sample.instruction, sample.prompt)\n",
        "                prompt_enc = tokenizer([prompt], return_tensors=\"pt\")\n",
        "                prompt_enc.to(\"cuda\")\n",
        "                model_output = model.generate(\n",
        "                    **prompt_enc,\n",
        "                    max_new_tokens=self.output_length,\n",
        "                    use_cache=False,\n",
        "                    output_hidden_states=False,\n",
        "                    output_attentions=False,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "                expected_answer = sample.chosen\n",
        "                expected_answer_enc = tokenizer(\n",
        "                    [expected_answer],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.output_length,\n",
        "                )\n",
        "                expected_answer_enc.to(\"cuda\")\n",
        "                expected_answer_embs = embeddings_model(**expected_answer_enc)\n",
        "                rejected_answer = sample.rejected\n",
        "                rejected_answer_enc = tokenizer(\n",
        "                    [rejected_answer],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.output_length,\n",
        "                )\n",
        "                rejected_answer_enc.to(\"cuda\")\n",
        "                rejected_answer_embs = embeddings_model(**rejected_answer_enc)\n",
        "\n",
        "                input_length = len(prompt_enc[\"input_ids\"][0])\n",
        "\n",
        "                # Slice the output to remove the input tokens\n",
        "                response_tokens = model_output[0][input_length:]\n",
        "\n",
        "                output_string = tokenizer.decode(\n",
        "                    response_tokens, skip_special_tokens=True\n",
        "                )\n",
        "                answer_enc = tokenizer(\n",
        "                    [output_string],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.output_length,\n",
        "                )\n",
        "                answer_enc.to(\"cuda\")\n",
        "                model_output_embs = embeddings_model(**answer_enc)\n",
        "                cosine_similarity_gain = get_cosine_embeddings(\n",
        "                    model_output_embs, expected_answer_embs\n",
        "                )\n",
        "                score_accumulated += cosine_similarity_gain.item()\n",
        "                cosine_similarity_loss = get_cosine_embeddings(\n",
        "                    model_output_embs, rejected_answer_embs\n",
        "                )\n",
        "                score_accumulated -= cosine_similarity_loss.item()\n",
        "\n",
        "                del (\n",
        "                    answer_enc,\n",
        "                    rejected_answer_enc,\n",
        "                    expected_answer_enc,\n",
        "                    prompt_enc,\n",
        "                    model_output_embs,\n",
        "                    expected_answer_embs,\n",
        "                    rejected_answer_embs,\n",
        "                    cosine_similarity_gain,\n",
        "                    cosine_similarity_loss,\n",
        "                )\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        performance = score_accumulated / (n_samples * len(datasets))\n",
        "        return performance\n",
        "\n",
        "    def assess_layers_snr(self, layer_types, layer_numbers):\n",
        "        for name, _ in self.model.named_modules():\n",
        "            for layer_number in layer_numbers:\n",
        "                for layer_type in layer_types:\n",
        "                    if layer_type in name and str(layer_number) in name:\n",
        "                        layer_name = f\"{layer_type}+{layer_number}\"\n",
        "                        print(\"*\" * 50, flush=True)\n",
        "                        print(\n",
        "                            f\"Calculating Signal to Noise Ratio at layer {layer_name}\",\n",
        "                            flush=True,\n",
        "                        )\n",
        "                        snr = self.calculate_snr_for_layer(layer_type, layer_number)\n",
        "                        self.layer_snr[layer_name] = snr\n",
        "                        print(\n",
        "                            f\"Signal to Noise Ratio at layer {layer_name} = {snr}\",\n",
        "                            flush=True,\n",
        "                        )\n",
        "                        print(\"*\" * 50, flush=True)\n",
        "\n",
        "    def select_layers_for_modification(self, k):\n",
        "        sorted_layers = sorted(\n",
        "            self.layer_snr.items(), key=lambda x: x[1], reverse=False\n",
        "        )\n",
        "        return [layer[0] for layer in sorted_layers[:k]]\n",
        "\n",
        "    def test_and_modify_layers(self, candidate_layers):\n",
        "        initial_performance = self.calculate_model_performance()\n",
        "\n",
        "        print(f\"Initial Model Performance: {initial_performance}\")\n",
        "\n",
        "        for layer in candidate_layers:\n",
        "            # Modify the layer\n",
        "            layer_type = layer.split(\"+\")[0]\n",
        "            layer_number = layer.split(\"+\")[1]\n",
        "            self.update_model_reduce_layer(\n",
        "                layer_type=layer_type, layer_number=layer_number\n",
        "            )\n",
        "\n",
        "            # Test the model's performance\n",
        "            new_performance = self.calculate_model_performance()\n",
        "            print(\n",
        "                f\"Tested Model Performance after modifying {layer}: {new_performance}\"\n",
        "            )\n",
        "\n",
        "            # If the performance does not improve, revert the change\n",
        "            if new_performance <= initial_performance:\n",
        "                self.restore_model_original_layer(\n",
        "                    layer_type=layer_type, layer_number=layer_number\n",
        "                )\n",
        "                print(\n",
        "                    f\"Reverted changes in {layer} due to lack of improvement.\",\n",
        "                    flush=True,\n",
        "                )\n",
        "            else:\n",
        "                initial_performance = new_performance\n",
        "                print(\n",
        "                    f\"Modification kept for {layer}. New baseline performance: {initial_performance}\",\n",
        "                    flush=True,\n",
        "                )\n",
        "\n",
        "    def save_model(self, save_dir):\n",
        "\n",
        "        self.model.save_pretrained(save_dir)\n",
        "        self.tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "\n",
        "# Usage\n",
        "model_name = \"openaccess-ai-collective/DPOpenHermes-7B-v2\"\n",
        "modifier = ModelModifier(model_name)\n",
        "\n",
        "# %%\n",
        "layer_numbers = list(range(31, -1, -1))\n",
        "layer_numbers = [f\".{l}.\" for l in layer_numbers]\n",
        "print(layer_numbers)\n",
        "\n",
        "layer_types=['mlp.gate', 'mlp.down_proj', 'mlp.up_proj', 'self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj']\n",
        "\n",
        "modifier.assess_layers_snr(layer_types, layer_numbers)\n",
        "top_k_layers = modifier.select_layers_for_modification(15)  # Select top 15 layers\n",
        "print(top_k_layers, flush=True)\n",
        "\n",
        "modifier.test_and_modify_layers(top_k_layers)\n",
        "# %%\n",
        "modifier.save_model(\"laser_model\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6vOzpS7KgV7"
      },
      "source": [
        "\n",
        "The key parts of this script are the **calculate_model_performance** and **calculate_snr_for_layer** functions:\n",
        "\n",
        "### Calculate Model Performance\n",
        "\n",
        "The `calculate_model_performance` method in the Python code evaluates the overall performance of the LLM. This method incorporates both the generation of model outputs and the comparison of these outputs with expected results using cosine similarity. \n",
        "\n",
        "Here's a step-by-step breakdown of the process:\n",
        "\n",
        "1. **Initialization**: The method begins by initializing a score accumulator to zero. This score accumulator will be used to compile the overall performance score of the model across multiple data sets and samples.\n",
        "\n",
        "2. **Benchmark Dataset**: For each specified dataset (e.g., \\\"orca_dpo\\\", \\\"ultrafeedback\\\"), the method retrieves a benchmark dataset. This dataset contains a number of samples (defined by `n_samples`) with a specific input length (`input_length`) and output length (`output_length`).\n",
        "\n",
        "3. **Sample Iteration**: The method iterates through each sample in the benchmark dataset. For each sample, it performs the following steps:\n",
        "\n",
        "   - **Prompt Creation**: Using the sample's instruction and prompt, it creates a suitable prompt for the model. \n",
        "\n",
        "   - **Prompt Encoding**: The created prompt is then encoded into a format that can be processed by the model.\n",
        "\n",
        "   - **Model Output Generation**: The method generates an output from the model based on the encoded prompt. The length of the output is defined by `output_length`.\n",
        "\n",
        "   - **Expected Answer Encoding**: The expected answer from the sample is also encoded in a format that can be processed by the model.\n",
        "\n",
        "   - **Rejected Answer Encoding**: The rejected answer from the sample (if any) is encoded similarly.\n",
        "\n",
        "   - **Embeddings Calculation**: The method calculates embeddings for the model output, expected answer, and rejected answer. These embeddings are computed using an embedding model, which transforms the raw encoded text into a dense vector representation.\n",
        "\n",
        "4. **Cosine Similarity Calculation**: The method calculates the cosine similarity between the embeddings of the model output and the expected answer. This similarity score measures how closely the model's output aligns with the expected answer. The score is added to the score accumulator. If a rejected answer is present, the method also calculates the cosine similarity between the model output and the rejected answer. This score is subtracted from the score accumulator.\n",
        "\n",
        "5. **Memory Management**: After processing each sample, the method clears the allocated memory for the various encodings and embeddings to optimize memory usage and prevent memory leaks.\n",
        "\n",
        "6. **Performance Calculation**: Once all samples in all datasets have been processed, the method calculates the overall performance of the model by dividing the total accumulated score by the total number of samples across all datasets.\n",
        "\n",
        "This comprehensive evaluation process allows for a detailed assessment of the model's performance, providing a basis for model optimization and refinement. It also provides a mechanism for comparing the performance of different models or different configurations of the same model\n",
        "\n",
        "\n",
        "### Calculate Signal-to-Noise-Ration for Layer\n",
        "The `calculate_snr_for_layer` method in the Python code performs a detailed analysis of the signal-to-noise ratio (SNR) for a specific layer within a neural network model. This method incorporates both the extraction of singular values from the layer's weights and the application of statistical measures to determine the layer's SNR. Here's a step-by-step breakdown of the process, integrating the mathematical concepts and formulas addressed previously:\n",
        "\n",
        "1. **Identify Layer Weights**: For a given layer type and number, the method iterates through the model's layers to find a match. Once found, it extracts the weights of the layer and converts them to double precision for accurate computation.\n",
        "\n",
        "2. **Singular Value Decomposition (SVD) Values**: The method calculates the singular values (\\(S\\)) of the layer's weight matrix using PyTorch's `torch.linalg.svdvals` function. This step is crucial for assessing the layer's information content through its singular values.\n",
        "\n",
        "3. **Maximum Singular Value**: It records the maximum singular value (\\(S[0]\\)), which represents the highest magnitude of signal strength in the layer's weights.\n",
        "\n",
        "4. **Estimate Sigma with IQR**: Using the full inter-quantile range (IQR) method, it estimates the standard deviation (\\(\\sigma\\)) of the singular values. This estimation helps in setting a threshold for distinguishing between signal and noise based on the variability of the singular values:\n",
        "   \\[\\sigma = \\frac{IQR}{1.349}\\]\\\n",
        "\n",
        "\n",
        "5. **Marchenko-Pastur Threshold**: The method then calculates the Marchenko-Pastur threshold (\\(\\lambda\\)) to separate the singular values into signal and noise categories. This threshold is computed using the formula:\n",
        "   \\[\\lambda = \\sigma \\sqrt{(1 + \\sqrt{\\beta})^2}\\]\n",
        "   where \\(\\beta\\) is the aspect ratio of the weight matrix (\\(n/m\\) or \\(m/n\\), whichever is smaller).\n",
        "\n",
        "6. **Signal and Noise Calculation**: The singular values greater than the Marchenko-Pastur threshold (\\(\\lambda\\)) are considered signal, and those below are considered noise. The method sums these groups of singular values separately to quantify the total signal (\\(\\sum_{\\sigma_i > \\lambda} \\sigma_i\\)) and total noise (\\(\\sum_{\\sigma_i \\leq \\lambda} \\sigma_i\\)).\n",
        "\n",
        "7. **Signal-to-Noise Ratio (SNR)**: The SNR is calculated by dividing the total signal by the total noise. In cases where the noise is zero (to avoid division by zero), the SNR is set to infinity (\\(\\infty\\)), indicating a layer with overwhelmingly dominant signal content.\n",
        "\n",
        "8. **SNR Ratio Relative to Maximum Singular Value**: The method further refines the SNR analysis by calculating the ratio of the SNR to the maximum singular value. This ratio provides insight into how the layer's strongest signal component compares to the overall signal-to-noise balance:\n",
        "   \\[SNR\\ Ratio = \\frac{SNR}{\\text{max singular value}}\\]\n",
        "\n",
        "9. **Memory Management**: After the calculations, the method clears the allocated memory for the singular values and weights to optimize memory usage and prevent memory leaks.\n",
        "\n",
        "This detailed analysis enables the identification of layers with high signal-to-noise efficiency, indicating layers that are potentially more influential or critical to the model's performance. Layers with higher SNR ratios relative to their maximum singular value are considered to have weights that are more effectively contributing to the model's output, providing a basis for model optimization and refinement.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQhmbvr-Wvwr"
      },
      "source": [
        "### **Results**\n",
        "\n",
        "![SP-Laser Benchmark](../assets/SP-Laser-benchmark.png)\n",
        "\n",
        "The results derived from HuggingFace's [Enterprise Scenarios Leaderboard](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard) Benchmark indicate distinct performance variations when applying the laser technique in two different ways. There is an observable overall enhancement in the benchmark performance. However, it's important to note that the degree of gains and losses fluctuates across the various benchmarks.\n",
        "\n",
        "In a general sense, it appears that the performance-based approach, applied on the dataset that the model was trained on, tends to yield superior results. This is closely followed by the perplexity-based laser approach. The base model, in comparison, tends to lag behind, registering the least effective performance.\n",
        "\n",
        "These observations underscore the potential of laser techniques in improving model performance, although the effectiveness can vary based on the specific approach and dataset used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPK5g6EsTkV"
      },
      "source": [
        "### **Future Work**\n",
        "\n",
        "Many more experiments need to be conducted. However, so far, we have been able to identify significant improvements compared to perplexity based approach.\n",
        "\n",
        "Curious to see the results others have on different models and datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qJhqNXDPPAuC",
        "mgIqJzUpJUoA",
        "Df2bWWgCOFHm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
